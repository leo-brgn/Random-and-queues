---
title: "Variables al√©atoires"
author: "Diana Nurbakova"
output:
  pdf_document: 
    toc: true
    toc_depth: 5
    latex_engine: pdflatex
    citation_package: biblatex
    keep_tex: true
  html_document: 
    css: style.css
    toc: true
    toc_float: true
    citation_package: biblatex
  includes:
      in_header: preamble.tex
always_allow_html: true
bibliography: bibliography.bib
cls: chicago-fullnote-bibliography.cls
header-includes:
- \usepackage{xcolor}
- \usepackage{framed}
- \usepackage{color}
- \usepackage{tcolorbox}
- \usepackage{kmath}
- \usepackage{amssymb}
---


```{r setup, include=FALSE}
options(width = 100)
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
options(knitr.table.format = "latex")
options(knitr.table.format = "html")

if(!require(devtools)) install.packages("devtools")
# if(!require(ztable)) install.packages("ztable")
devtools::install_github("cardiomoon/ztable")
devtools::install_github("lbusett/insert_table")

# webshot::install_phantomjs()



# color text
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

cardSuits <- function(s, col="black") {
#  col <- "black"
  if (s == "hearts"){
    xLatex <- "$\\varheartsuit$"
    xHTML <- "&hearts;"
    col <- "red"
  } else if (s == "diamonds"){
    xLatex <- "$\\vardiamondsuit$"
    xHTML <- "&diams;"
    col <- "red"
  } else if (s == "clubs"){
    xLatex <- "$\\clubsuit$"
    xHTML <- "&clubs;"
  } else if (s == "spades"){
    xLatex <- "$\\spadesuit$"
    xHTML <- "&spades;"
  } else {
    xLatex <- s
    xHTML <- s
  }
    
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", col, xLatex)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", col, xHTML)
  } else s
}

exmplPair <- function(){
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{red}{J$\\varheartsuit$} J$\\clubsuit$ K$\\clubsuit$ 9$\\spadesuit$ \\textcolor{red}{3$\\varheartsuit$}")
  } else if (knitr::is_html_output()) {
    sprintf("<span style='font-size:100px; color:red'>&#x1F0BB; </span>
             <span style='font-size:100px;'>&#x1F0DB;  &#x1F0DE; &#x1F0A9; </span> 
             <span style='font-size:100px; color:red'> &#x1F0B3;</span>")
  }  
    
}

exmplCards <- function(rang, s="spades", size="40", col="black"){
  if (s == "hearts"){
    xLatex <- "$\\varheartsuit$"
    col <- "red"
    suitSuf <- "B"
  } else if (s == "diamonds"){
    xLatex <- "$\\vardiamondsuit$"
    col <- "red"
    suitSuf <- "C"
  } else if (s == "clubs"){
    xLatex <- "$\\clubsuit$"
    suitSuf <- "D"
  } else if (s == "spades"){
    xLatex <- "$\\spadesuit$"
    suitSuf <- "A"
  } else {
    xLatex <- s
    xHTML <- s
  }
  
  if (rang == "A"){
      suf <- "1"
  } else if (rang %in% c("2", "3", "4", "5", "6", "7", "8", "9")){
    suf <- rang
  } else if (rang == "10"){
    suf <- "A"
  } else if (rang == "J"){
    suf <- "B"
  } else if (rang == "Q"){
    suf <- "D"
  } else if (rang == "K"){
    suf <- "E"
  } else {
    suf <- rang
  }
  
  xHTML = paste("&#x1F0", suitSuf, suf, ";", sep="")
  
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s%s}", col, rang, xLatex)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='font-size:%spx; color:%s'>%s</span>", size, col, xHTML)
  }  
  
  
}

gifImgCollection <- function(imgPath, nb=6){
  if (knitr::is_latex_output()) {
    #for (i in seq(0,nb)){
    #  img <- paste(imgPath, "-", i, ".png", sep="")
    #  #sprintf("\\includegraphics[width=0.5\textwidth]{%s}", img)
    #  sprintf("![](%s)", img)
    #  print(img)
    #}
    sprintf("![](%s)", paste(imgPath, ".png", sep=""))
    #sprintf("![](img/coffee-0.png)")
  } else if (knitr::is_html_output()) {
    img <- paste(imgPath, "-gif.gif", sep="")
    sprintf("![](%s)", img)
  } 
}


getZtable <- function(){
  if (knitr::is_html_output()){
    rowH <- as.character(seq(0,4,0.1))
    rowH[1] <- "0.0"
    rowH[11] <- "1.0"
    rowH[21] <- "2.0"
    rowH[31] <- "3.0"
    rowH[41] <- "4.0"
    z_tbl <- tibble::tribble(
      ~"+0.00", ~"+0.01", ~"+0.02", ~"+0.03", ~"+0.04", ~"+0.05", ~"+0.06", ~"+0.07", ~"+0.08", ~"+0.09",
      0.50000, 0.50399, 0.50798, 0.51197, 0.51595, 0.51994, 0.52392, 0.52790, 0.53188, 0.53586, 
      0.53983, 0.54380, 0.54776, 0.55172, 0.55567, 0.55962, 0.56360, 0.56749, 0.57142, 0.57535, 
      0.57926, 0.58317, 0.58706, 0.59095, 0.59483, 0.59871, 0.60257, 0.60642, 0.61026, 0.61409, 
      0.61791, 0.62172, 0.62552, 0.62930, 0.63307, 0.63683, 0.64058, 0.64431, 0.64803, 0.65173, 
      0.65542, 0.65910, 0.66276, 0.66640, 0.67003, 0.67364, 0.67724, 0.68082, 0.68439, 0.68793, 
      0.69146, 0.69497, 0.69847, 0.70194, 0.70540, 0.70884, 0.71226, 0.71566, 0.71904, 0.72240, 
      0.72575, 0.72907, 0.73237, 0.73565, 0.73891, 0.74215, 0.74537, 0.74857, 0.75175, 0.75490, 
      0.75804, 0.76115, 0.76424, 0.76730, 0.77035, 0.77337, 0.77637, 0.77935, 0.78230, 0.78524, 
      0.78814, 0.79103, 0.79389, 0.79673, 0.79955, 0.80234, 0.80511, 0.80785, 0.81057, 0.81327, 
      0.81594, 0.81859, 0.82121, 0.82381, 0.82639, 0.82894, 0.83147, 0.83398, 0.83646, 0.83891, 
      0.84134, 0.84375, 0.84614, 0.84849, 0.85083, 0.85314, 0.85543, 0.85769, 0.85993, 0.86214, 
      0.86433, 0.86650, 0.86864, 0.87076, 0.87286, 0.87493, 0.87698, 0.87900, 0.88100, 0.88298, 
      0.88493, 0.88686, 0.88877, 0.89065, 0.89251, 0.89435, 0.89617, 0.89796, 0.89973, 0.90147, 
      0.90320, 0.90490, 0.90658, 0.90824, 0.90988, 0.91149, 0.91308, 0.91466, 0.91621, 0.91774, 
      0.91924, 0.92073, 0.92220, 0.92364, 0.92507, 0.92647, 0.92785, 0.92922, 0.93056, 0.93189, 
      0.93319, 0.93448, 0.93574, 0.93699, 0.93822, 0.93943, 0.94062, 0.94179, 0.94295, 0.94408, 
      0.94520, 0.94630, 0.94738, 0.94845, 0.94950, 0.95053, 0.95154, 0.95254, 0.95352, 0.95449, 
      0.95543, 0.95637, 0.95728, 0.95818, 0.95907, 0.95994, 0.96080, 0.96164, 0.96246, 0.96327, 
      0.96407, 0.96485, 0.96562, 0.96638, 0.96712, 0.96784, 0.96856, 0.96926, 0.96995, 0.97062, 
      0.97128, 0.97193, 0.97257, 0.97320, 0.97381, 0.97441, 0.97500, 0.97558, 0.97615, 0.97670, 
      0.97725, 0.97778, 0.97831, 0.97882, 0.97932, 0.97982, 0.98030, 0.98077, 0.98124, 0.98169, 
      0.98214, 0.98257, 0.98300, 0.98341, 0.98382, 0.98422, 0.98461, 0.98500, 0.98537, 0.98574, 
      0.98610, 0.98645, 0.98679, 0.98713, 0.98745, 0.98778, 0.98809, 0.98840, 0.98870, 0.98899, 
      0.98928, 0.98956, 0.98983, 0.99010, 0.99036, 0.99061, 0.99086, 0.99111, 0.99134, 0.99158, 
      0.99180, 0.99202, 0.99224, 0.99245, 0.99266, 0.99286, 0.99305, 0.99324, 0.99343, 0.99361, 
      0.99379, 0.99396, 0.99413, 0.99430, 0.99446, 0.99461, 0.99477, 0.99492, 0.99506, 0.99520, 
      0.99534, 0.99547, 0.99560, 0.99573, 0.99585, 0.99598, 0.99609, 0.99621, 0.99632, 0.99643, 
      0.99653, 0.99664, 0.99674, 0.99683, 0.99693, 0.99702, 0.99711, 0.99720, 0.99728, 0.99736, 
      0.99744, 0.99752, 0.99760, 0.99767, 0.99774, 0.99781, 0.99788, 0.99795, 0.99801, 0.99807, 
      0.99813, 0.99819, 0.99825, 0.99831, 0.99836, 0.99841, 0.99846, 0.99851, 0.99856, 0.99861, 
      0.99865, 0.99869, 0.99874, 0.99878, 0.99882, 0.99886, 0.99889, 0.99893, 0.99896, 0.99900, 
      0.99903, 0.99906, 0.99910, 0.99913, 0.99916, 0.99918, 0.99921, 0.99924, 0.99926, 0.99929, 
      0.99931, 0.99934, 0.99936, 0.99938, 0.99940, 0.99942, 0.99944, 0.99946, 0.99948, 0.99950, 
      0.99952, 0.99953, 0.99955, 0.99957, 0.99958, 0.99960, 0.99961, 0.99962, 0.99964, 0.99965, 
      0.99966, 0.99968, 0.99969, 0.99970, 0.99971, 0.99972, 0.99973, 0.99974, 0.99975, 0.99976, 
      0.99977, 0.99978, 0.99978, 0.99979, 0.99980, 0.99981, 0.99981, 0.99982, 0.99983, 0.99983, 
      0.99984, 0.99985, 0.99985, 0.99986, 0.99986, 0.99987, 0.99987, 0.99988, 0.99988, 0.99989, 
      0.99989, 0.99990, 0.99990, 0.99990, 0.99991, 0.99991, 0.99992, 0.99992, 0.99992, 0.99992, 
      0.99993, 0.99993, 0.99993, 0.99994, 0.99994, 0.99994, 0.99994, 0.99995, 0.99995, 0.99995, 
      0.99995, 0.99995, 0.99996, 0.99996, 0.99996, 0.99996, 0.99996, 0.99996, 0.99997, 0.99997, 
      0.99997, 0.99997, 0.99997, 0.99997, 0.99997, 0.99997, 0.99998, 0.99998, 0.99998, 0.99998,
      )
    
    require(rhandsontable)
    rhandsontable(z_tbl, rowHeaders = rowH,
                   digits = 3, useTypes = FALSE, search = FALSE,
                   width = 900, height = 1000)

  } else if (knitr::is_latex_output()){
    sprintf("![](img/z-table.png)")
  }
}


```



\setlength{\fboxsep}{.8em}

\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}
\definecolor{bluegray}{rgb}{0.4, 0.6, 0.8}
\definecolor{glaucous}{rgb}{0.38, 0.51, 0.71}
\definecolor{hanblue}{rgb}{0.27, 0.42, 0.81}
\definecolor{mediumpersianblue}{rgb}{0.0, 0.4, 0.65}
\definecolor{sapphire}{rgb}{0.03, 0.15, 0.4}
\definecolor{cambridgeblue}{rgb}{0.64, 0.76, 0.68}
\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}
\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}
\definecolor{darkseagreen}{rgb}{0.56, 0.74, 0.56}
\definecolor{darkolivegreen}{rgb}{0.33, 0.42, 0.18}
\definecolor{charcoal}{rgb}{0.21, 0.27, 0.31}
\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}
\definecolor{feldgrau}{rgb}{0.3, 0.36, 0.33}
\definecolor{grannysmithapple}{rgb}{0.66, 0.89, 0.63}
\definecolor{lightgreen}{rgb}{0.56, 0.93, 0.56}
\definecolor{magicmint}{rgb}{0.67, 0.94, 0.82}
\definecolor{mossgreen}{rgb}{0.68, 0.87, 0.68}
\definecolor{sandstorm}{rgb}{0.93, 0.84, 0.25}
\definecolor{stildegrainyellow}{rgb}{0.98, 0.85, 0.37}
\definecolor{beaver}{rgb}{0.62, 0.51, 0.44}
\definecolor{burntumber}{rgb}{0.54, 0.2, 0.14}

\newtcolorbox{blackbox}{
  colback=black,
  colframe=orange,
  coltext=white,
  boxsep=5pt,
  arc=4pt}
  

\newtcolorbox{lightbluebox}{
  colback=aliceblue,
  colframe=aliceblue,
  coltext=sapphire,
  boxsep=5pt,
  arc=4pt}

\newtcolorbox{lightgreenbox}{
  colback=mossgreen!50!white,
  colframe=mossgreen,
  coltext=darkgreen,
  boxsep=5pt,
  arc=4pt}
  
\newtcolorbox{lightgreenframe}{
  colback=white,
  colframe=mossgreen,
  coltext=darkgreen,
  boxsep=5pt,
  arc=4pt}
  
\newtcolorbox{lightyellowbox}{
  colback=stildegrainyellow!40!white,
  colframe=sandstorm,
  coltext=burntumber,
  boxsep=5pt,
  arc=4pt}
  
\newtcolorbox{lightyellowframe}{
  colback=white,
  colframe=sandstorm,
  coltext=burntumber,
  boxsep=5pt,
  arc=4pt}


# Variable al√©atoire : √©ventualit√©-gain

<center>
\centering

![](img/rv.png){width=75%}
</center>

\raggedright

Imaginons qu'on met en place une exp√©riance al√©atoire en lan√ßant une pi√®ce. Les issues possibles sont $pile$ et $face$, autrement dit, l'univers est $\Omega = \{pile, face\}$. Ce dernier est plut√¥t abstrait et une telle repr√©sentation n'est pas toujours pratique pour les calculs. 

Nous aimerions donc de ``quantifier" ces issues, en faisant une correspondance de ces issues aux nombres. L'application des issues √† ce nouvel espace est une variable al√©atoire. Ainsi, une variable al√©atoire peut √™tre consid√©r√©e comme un r√©sultat d'une exp√©rience al√©atoire qu'on peut mesurer ou compter. 

Notons que dans ce contexte, le terme *variable* est diff√©rent de celui de l'alg√®bre. Une variable al√©atoire peut prendre plusieurs valeurs avec diff√©rentes probabilit√©s. 

Notons aussi qu'une variable al√©atoire peut √™tre plus "compl√®xe" qu'un recodage directe d'issues en nombres, e.g. la somme de r√©sultats de 5 lancers d'un d√© cubique  

<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

Soit $(\Omega, \mathcal{A})$ un espace d'ev√®nements de l'espace probabilis√© $(\Omega, \mathcal{A}, \mathbb{P})$ et $(E, \mathcal{E})$ un espace mesurable.  Une **variable al√©atoire, v.a.** (en. *random variable, r.v.*) $X$ de $\Omega$ vers $E$ est une fonction mesurable :
$$X : \Omega \rightarrow E$$ 

telle que : 

$$\forall A' \in \mathcal{E}, X^{-1}(A') \in \mathcal{A}$$

o√π $X^{-1}(A')$ est l'image r√©ciproque $X^{-1}(A') = \{\omega\in \Omega | X(\omega)\in A'\}\in \mathcal{A}$.

Souvent, on va consid√©rer le cas o√π $E \subset \mathbb{R}$. Une **variable al√©atoire r√©elle, v.a.r.** (en. *real-valued random variable*) sur l'espace d'√©v√®nements $(\Omega, \mathcal{A})$ est une fonction mesurable $X : \Omega \rightarrow \mathbb{R}$, telle que :
$$\forall x\in \mathbb{R}, X^{-1}(]-\infty, x])\in \mathcal{A}$$

::::
</div>

**Remarque :** Pour la notation de variables al√©atoires, les lettres majuscules sont utilis√©es par la convention.

> Quelles sont les valeurs de la v.a. $X$ ?

Selon la valeur qu'une variable al√©atoire peut prendre, on distingue les diff√©rents types de variables al√©atoires. 

<center>

\centering

![](img/rv-types.png){width=75%}
</center>

\raggedright

### Variable al√©atoire discr√®te

<div class="alert alert-success">

:::: {.lightgreenbox data-latex=""}

Une v.a.r. $X$ est appel√©e **discr√®te** (en. *discrete r.v.*) si elle ne prend qu'un nombre d√©nombrable et/ou fini de valeurs dans $\mathbb{R}$, i.e. $X(\Omega) = \left\{x_j\in \mathbb{R}, j\in J\right\}$ avec $J \subset \mathbb{N}$.

::::
</div>

Voici quelques exemples : $X =$ "*Le nombre de posts TikTok publi√©s dans la prochaine heure*", $X =$ "*Le taux d'h√©moglobine dans le sang de la population*".

### Variable al√©atoire continue

<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

Dans le cas g√©n√©rale, une v.a.r. $X$ est appel√©e **continue** (en. *continuous r.v.*) si son image est ind√©nombrablement infinie (souvent un intervalle).

Dans le cas plus concret, il s'agit de la *continuit√© absolue* (au sens de Lebesgue). 

::::
</div>

Voici quelques exemples : $X =$ "*Le poids (exacte) d'un(e) √©tudiant(e) de l'INSA pris(e) au hasard*", $X =$ "*La hauteur (exacte) franchie par l'athl√®te gagnant les sauts √† la perche lors de prochains Jeux Olympiques*", $X =$ "*Le niveau de r√©chauffement climatique atteint vers 2030*".

# Fonction de r√©partition et Loi d'une variable al√©atoire r√©elle

<div class="alert alert-success" style='background-color:white'>

:::: {.lightgreenframe data-latex=""}

Un espace $(\Omega, \mathcal{A})$ est munie d'une mesure de probabilit√© $\mathbb{P}$ dans l'espace probabilis√© $(\Omega, \mathcal{A}, \mathbb{P})$. Gr√¢ce √† la m√©surabilit√© de la variable al√©atoire $X$, il est possible de d√©finir la probabilit√© $\mathbb{P}_X: \mathcal{E} \rightarrow [0,1]$ (aussi appel√©e **la loi de probabilit√© de la v.a.r. $X$** ou juste **loi de la v.a.r. $X$** (en. *probability distribution*)) sur l'espace mesurable $(E, \mathcal{E})$ :
$$\forall A'\in \mathcal{E}, \ \ \mathbb{P}_X(A') = \mathbb{P}(X^{-1}(A')) = \mathbb{P}(X\in A')$$

Ainsi, une loi de probabilit√© est une fonction qui d√©crit la probabilit√© d'occurrence d'issues possibles de l'exp√©rience al√©atoire. 

::::
</div>

> Comment est-ce que la v.a.r. $X$ est r√©parti ?

Pour d√©finir la r√©partition d'une v.a.r. les diff√©rentes repr√©sentations peuvent √™tre utilis√©s comme : des graphes, des tables (surtout dans le cas d'une v.a. discr√®te), des fonctions. 

<center>

\centering

![](img/distribution-ways.png){width=75%}
</center>

\raggedright

Dans le cas g√©n√©ral, on peut parler de la fonction de r√©partition qui caract√©rise la loi de probabilit√©.

<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

Soit $X$ une v.a.r. sur l'espace probabilis√© $(\Omega, \mathcal{A}, \mathbb{P})$. On appelle **la fonction de r√©partition d'une v.a.r. $X$** (en. *cumulative distribution function*) ou  **la fonction de distribution cumulative d'une v.a.r. $X$** l'application $F_X$ qui √† $\forall x\in \mathbb{R}$ associe la probabilit√© d'obtenir une valeur inf√©rieure ou √©gale √† $x$, i.e. :
$$F_X: \ \left.
    \begin{array}{ll}
        \mathbb{R}\rightarrow [0,1]  \\
        x \rightarrow \mathbb{P}(X^{-1}(]-\infty,x]))
    \end{array}
\right.
$$
Autrement dit, $F_X(x) = \mathbb{P}(X\leq x)$. 

::::
</div>

<div class="alert alert-success" style='background-color:white'>
:::: {.lightgreenframe data-latex=""}

Les propri√©t√©s de base de la fonction de r√©partition d'une v.a.r. :

* $F_X$ est toujours croissante, i.e. $\forall (a,b)\in \mathbb{R}^2,\ a \leq b \Rightarrow F_X(a) \leq F_X(b)$ 
* $F_X$ est continue √† droite 
* $\lim\limits_{x\rightarrow-\infty} F_X(x) = 0$ et $\lim\limits_{x\rightarrow+\infty} F_X(x) = 1$

Notons que $F_X$ est une fonction monotonne born√©e :
$$\forall x\in \mathbb{R}, \ 0\leq F_X(x)\leq 1$$

::::
</div>

<div class="alert alert-success" style='background-color:white'>
:::: {.lightgreenframe data-latex=""}

La fonction de r√©partition permet de calculer la probabilit√© d'une v.a.r. $X$ d'√™tre incluse dans un intervalle semi-ouvert √† gauche $]a,b]$ o√π $a < b$ comme suit :
$$\mathbb{P}(X \in ]a,b]) = \mathbb{P}(a < X \leq b) = F_X(b)-F_X(a)$$

::::
</div>


## Variables discr√®tes 

### Fonction de masse
Supposons qu'on s'int√©resse au nombre de tasses de caf√© qu'un √©tudiant prend avant la pause midi. 
Dans le tableau, on pr√©sente toutes les valeurs √† probabilit√© non nulle. Dans notre cas, ce sont : $1$, $2$, $3$, $4$, $5$. Supposons aussi qu'on connait la probabilit√© de chacune de ses valeurs. Ecrivons-les dans le m√™me tableau. 

<center>

\centering

![](img/coffee-init.png){width=50%}

</center>


\raggedright

<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

Soit $X$ une v.a.r. discr√®te, $X(\Omega) = \left\{x_j\in \mathbb{R}, j\in J\right\}$ avec $J \subset \mathbb{N}$. **La fonction de masse de la v.a.r. $X$** (en. *probability mass function, pmf*) est une application $p$ telle que : 
$$p \ \left.
    \begin{array}{ll}
        J\rightarrow [0,1]  \\
        j \rightarrow p_j
    \end{array}
\right.$$
o√π $p_j = \mathbb{P}(X = x_j)$, en consid√©rant que $\forall j \in \mathbb{N}\setminus J, \ p_j = 0$.

La fonction de masse $p$ a des propri√©t√©s suivantes :

* $\forall j\in J, \ p_j \geq 0,\ p_j \in [√†,1]$
* $\sum\limits_{j \in \mathbb{N}} p_j = 1$

::::
</div>

V√©rifions que les valeurs pr√©sent√©es dans le tableau forment une fonction de masse valide :

$$\sum_{i=1}^n p(x_i) = 0.4 + 0.25 + 0.2 + 0.1 + 0.05 = \mathbf{1}$$
et toutes $p_j \geq 0$.

Les conditions sont bien satisfaites.

On peut repr√©senter la fonction de masse de $X$ sous forme d'un graphique :

<center>

\centering

![](img/coffee-pmf-plot.png)


</center>

\raggedright

### Fonction de r√©partition

Regardons maintenant comment on peut d√©finir la fonction de r√©partition d'une v.a.r. discr√®te.

Comme pr√©cis√© avant, la fonction de r√©partition est une fonction cumulative. Dans le cas d'une v.a.r. discr√®te :

<center>

\centering

`r gifImgCollection(imgPath="img/coffee", nb=6)`

</center>


\raggedright

Ainsi, nous obtenons les valeurs suivantes :

<center>

\centering

![](img/coffee-6.png){width=50%}


</center>

\raggedright

<div class="alert alert-success" style='background-color:white'>

:::: {.lightgreenframe data-latex=""}

Soit $X$ une v.a.r. discr√®te. Pour tout $k \in \mathbb{N}$ :
$$F_X(x) = \sum_{i=1}^k p_i$$

::::
</div>

Repr√©sentons la fonction de r√©partition sous forme de graphique :

<center>

\centering

![](img/coffee-cdf-plot.png)


</center>


\raggedright

La fonction de r√©partition d'une v.a.r. discr√®te est constante par morceaux et admet les points de discontinuit√© $\{x_j, j\in J\}$

<center>

\centering

![](img/coffee-cdf-plot-2.png){width=50%}

</center>


\raggedright

#### Caract√©risation de la loi d'une v.a.r. discr√®te par la fonction de masse

Notons que la fonction de masse $p$ caract√©rise la loi d'une v.a.r. discr√®te $X$. Dans le cas d'une v.a.r. discr√®te $X$ d√©finie sur $(\Omega, \mathcal{A}, \mathbb{P})$ :
$$\mathbb{P}_X(A) = \sum_{j\in \mathbb{N}}\left(\mathbb{P}(X=x_j)\times \mathbb{I}_A(x_j)\right) = \sum_{j\in \mathbb{N}}\left(p_j\times \mathbb{I}_A(x_j)\right)$$
o√π $A$ est un bor√©lien de $\mathbb{R}$, $\mathbb{I}_A(x) = \left\{ \begin{array}{ll} 1 \mbox{ si } x\in A  \\ 0 \mbox{ si } x\notin A \end{array}\right.$ est la fonction indicatrice de l'ensemble $A$.

## Variables continues

### Fonction de densit√©

<div class="alert alert-success">

:::: {.lightgreenbox data-latex=""}
Soit $X$ un v.a.r. absolument continue. **La fonction de densit√©** ou **densit√© de probabilit√©** (en. *probability density function, pdf*) $f_X(x)$ est une fonction positive et int√©grable sur $\mathbb{R}$, telle que :
$$\mathbb{P}(a\leq X\leq b) = \int_{a}^{b}f(t)dt$$ 

<center>

\centering

![](img/cont-rv-Pab.png)


</center>

\raggedright

Notons que : 
$$f_X(x) = \frac{d}{dx}F_X(x)$$
presque partout.

Les propri√©t√©s :

* $\forall t\in \mathbb{R}, \ f_X(t)\in \mathbb{R}^+$
* $\int_{\mathbb{R}}f(t)dt = 1$

::::
</div>


### Fonction de r√©partition

$$F_X(x) = \int_{-\infty}^{x}f_X(t)dt$$

Ainsi, la fonction de r√©partition $F_X(x)$ correspond √† l'aire sous la courbe $f_X(x)$ :

<center>

\centering

![](img/pdf-cont.png)


</center>

\raggedright

<center>

\centering

![](img/continuous-pdf-all.png){width=50%}

</center>


\raggedright

Similairement √† la recherche de la probabilit√© de l'√©v√®nement contraire, on peut trouver la probabilit√© $\mathbb{P}(X > a)$ comme suit :
$$\mathbb{P}(X > a) = 1 - \mathbb{P}(X \leq a)$$

<center>

\centering

![](img/normal-right.png)

</center>

\raggedright

#### Caract√©risation de la loi d'une v.a.r. continue par la fonction de densit√©

Notons que la fonction de densit√© $f(x)$ caract√©rise la loi d'une v.a.r. absolument continue $X$. Ainsi, dans le cas d'une v.a.r. discr√®te $X$ d√©finie sur $(\Omega, \mathcal{A}, \mathbb{P})$ avec $A$ √©tant un bor√©lien de $\mathbb{R}$:
$$\mathbb{P}_X(A) = \int_{A}f_X(t)dt = \int_{\mathbb{R}}f_X(t)\times \mathbb{I}_A(t)dt$$

### Exemple de calcul

Prenons l'exemple propos√© dans [@orloff2014].

La v.a. $X$ est d√©finie sur l'intervalle $[0,2]$ par la fonction de densit√© $f(x) = cx^2$. 

1. Quelle est la valeur de $c$ ?
2. Quelle est la fonction de r√©partition $F(x)$ ?
3. Quelle est la probabilit√© $\mathbb{P}(1\leq X \leq 2)$ ?

**Solution**

#### 1. Quelle est la valeur de $c$ ?

La somme des probabilit√©s sur tout l'intervalle $[a,b]$ sur lequel $X$ est d√©fini doit √™tre $√©gale √† 1$, i.e. $\int_a^b f(x)dx = 1$. Dans notre cas, $X$ est d√©fini sur $[0,2]$. Alors :

$$\int_a^b f(x)dx = \int_0^2 cx^2 dx = c\frac{x^3}{3}\Bigg\rvert_{0}^2 = c\frac{8}{3} - 0 = c\frac{8}{3} = 1$$
D'o√π $c = \mathbf{\frac{3}{8}}$.

En rempla√ßant $c$ par cette valeur, on obtient $f(x) = \frac{3}{8}x^2$.

#### 2. Quelle est la fonction de r√©partition $F(x)$ ?

Pour rappel, la fonction de r√©partition est donn√© par $F(x) = \mathbb{P}(X\leq x) = \int_{-\infty}^x f(t)dt$. Ainsi, en sachant $f(x)$, on peut trouver $F(x)$.

Notons que par d√©finition, la fonction de densit√© $f(x)=0$ en dehors de l'intervalle sur lequel $X$ est d√©fini.  Dans notre cas, on peut r√©√©crire $f(x)$ de la fa√ßon suivante :
$$f(x) = \left\{ \begin{array}{ll} 0, \mbox{ si } x < 0 \\ \frac{3}{8}x^2, \mbox{ si } x\in [0,2] \\ 0, \mbox{ si } x>2 \end{array} \right.$$
Donc, la fonction de r√©partition $F(x) = 0, \mbox{ si } x<0$ et $F(x) = 1, \mbox{ si } x>2$. Il reste donc trouver la fonction de r√©partition pour $x\in [0, 2]$.

$$F(x) = \int_{-\infty}^x f(t)dt = \int_{\mathbf{0}}^x ct^2dt = c\frac{t^3}{3}\Bigg\rvert_{0}^x = c\frac{x^3}{3} = \frac{3}{8}\cdot\frac{x^3}{3} = \frac{x^3}{8} = \left(\frac{x}{2}\right)^3$$
Alors, en rassemblant tout :

$$F(x) = \left\{ \begin{array}{ll} 0, \mbox{ si } x < 0 \\ \left(\frac{x}{2}\right)^3, \mbox{ si } x\in [0,2] \\ 1, \mbox{ si } x>2 \end{array} \right.$$

#### 3. Quelle est la probabilit√© $\mathbb{P}(1\leq X \leq 2)$ ?
On peut aborder le calcul de $\mathbb{P}(a\leq X\leq b)$ de deux fa√ßons :

1. $\mathbb{P}(a\leq X\leq b) = \int_a^b f(x)dx$
2. $\mathbb{P}(a\leq X\leq b) = F(b) - F(a)$

Essayons les deux options.

##### 3.1. Option 1 : $\mathbb{P}(a\leq X\leq b) = \int_a^b f(x)dx$

$$\mathbb{P}(1\leq X\leq 2) = \int_1^2 cx^2 dx = c\frac{x^3}{3}\Bigg\rvert_{1}^2 = \frac{3}{8}\cdot\frac{x^3}{3}\Bigg\rvert_{1}^2 = \frac{x^3}{8}\Bigg\rvert_{1}^2 = \frac{8}{8} - \frac{1}{8} = \mathbf{\frac{7}{8}}$$

##### 3.2. Option 2 : $\mathbb{P}(a\leq X\leq b) = F(b) - F(a)$

Notons que $1\in [0,2]$ et $2\in [0,2]$. Sur l'intervalle en question, la fonction de r√©partition prend la forme $F(x) = \left(\frac{x}{2}\right)^3$. Donc : 

$$\mathbb{P}(1\leq X\leq 2) = F(2) - F(1) = \left(\frac{2}{2}\right)^3 - \left(\frac{1}{2}\right)^3 = 1 - \frac{1}{8} = \mathbf{\frac{7}{8}}$$

## Bilan
R√©sumons les notions de la loi de probabilit√© de la fa√ßon suivante :

<center>

\centering

![](img/summary-distribution.png)


</center>

\raggedright


# Moments d'une variable al√©atoire

Il existe des caract√©ristiques num√©riques de la distribution d'une v.a.r. qu'on appelle des **moments** (en. *moments*). 
On va parler des moments qui montre le caract√®re central (en. *central tendency*) de la distribution (esp√©rance et m√©diane) et sa dispersion (variance et √©cart-type). 

<center>

\centering

![](img/moments.png){width=60%}


</center>

\raggedright

## Esp√©rance 

<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

**L'esp√©rance** (en. *expectation* ou *expected value* ou *mean* ou *average* ou *first moment*) d'une v.a.r. $X$, not√©e $\mathbb{E}X$ ou $\mathbb{E}[X]$ ou $\mathbb{E}(X)$ est une g√©n√©ralisation de la valeur moyenne pond√©r√©e. 

Son calcul (lorsque cette quantit√© existe) d√©pend de la nature de $X$ :

* v.a.r. discr√®te :
$$\mathbb{E}[X] = \sum_i x_ip_i = \sum_i x_iP(X=x)$$
* v.a.r. continue :
$$\mathbb{E}[X] = \int_{-\infty}^{+\infty}tf(t)dt$$
Notons que ces d√©finitions peuvent √™tre g√©n√©ralis√©es pour l'esp√©rance d'une v.a.r. $g(X)$, o√π $g : X(\Omega) \rightarrow \mathbb{R}$ :

* cas dicret : $\mathbb{E}[g(X)] = \sum_i g(x_i)p_i = \sum_i g(x_i)P(X=x)$
* cas contiue : $\mathbb{E}[g(X)] = \int_{-\infty}^{+\infty}g(t)f(t)dt$

::::
</div>

<div class="alert alert-success" style='background-color:white'>
:::: {.lightgreenframe data-latex=""}

**Propri√©t√©s :** 

* $\mathbb{E}[X] \geq 0$
* $\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y]$
* $\mathbb{E}[aX + b] = a\mathbb{E}[X] + b$

Notons que ces r√©sultats peuvent √™tre g√©n√©ralis√©s pour l'esp√©rance d'une v.a.r. $g(X)$, o√π $g : X(\Omega) \rightarrow \mathbb{R}$ : 

* $\mathbb{E}[g(X)] \geq 0$
* $\mathbb{E}[g_1(X) + g_2(X)] = \mathbb{E}[g_1(X)] + \mathbb{E}[g_2(X)]$
* $\mathbb{E}[ag(X)] = a\mathbb{E}[g(X)] + b$
* soit $X$ une v.a.r. continue, $g_1$ et $g_2$ deux fontions t.q. $g_1 \leq g_2$, alors $\mathbb{E}[g_1(X)] \leq \mathbb{E}[g_2(X)]$
* si $X$ est une v.a.r. constante sur $\Omega$ et $g$ une fonction quelconque, alors $\mathbb{E}[g(X)] = g(X)$

Les d√©monstrations de ces propri√©t√©s peuvent √™tre trouv√©es dans @balac.

::::
</div>

Une autre propri√©t√© peut √™tre formul√©e sous forme de l'*in√©galit√© de Markov* :

<div class="alert alert-warning">
:::: {.lightyellowbox data-latex=""}

**In√©galit√© de Markov** : Soit $X$ une v.a.r. Alors :
$$\forall a> 0, a\in \mathbb{R} :  \ \  \mathbb{P}(|X|\geq a) \leq \frac{1}{a} \mathbb{E}[|X|]$$

::::
</div>

**Exemple :** Si chaque fois que vous obtenez *face* en lan√ßant une pi√®ce, vous gagnez 5 euros, et chaque fois que vous obtenez *pile*, vous perdez 5 euros. Quelle est le gain moyen ou autrement dit, l‚Äôesp√©rance de gain ? 

$$\mathbb{E}[X] = \sum_i x_ip_i = \sum_i x_iP(X=x) = 5\times \frac{1}{2} + (-5)\times \frac{1}{2} = \mathbf{0}$$
En moeyenne, on gagne rien.


## Variance et √©cart-type

<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

**La variance** (en. *variance*) de la v.a.r. $X$, not√©e $Var(X)$ ou $\sigma^2$, est une mesure de la dispersion de donn√©es autour de son esp√©rance $\mathbb{E}X$. 

Son calcul (lorsque cette quantit√© existe) d√©pend de la nature de $X$ :

* v.a.r. discr√®te : 
$$Var(X) = \mathbb{E}[(X-\mathbb{E}X)^2]=\mathbb{E}[X^2]-(\mathbb{E}X)^2=\sum_i (x_i-\mathbb{E}X)^2 p_i$$

* v.a.r. continue :
$$Var(X) = \mathbb{E}[(X-\mathbb{E}X)^2]=\mathbb{E}[X^2]-(\mathbb{E}X)^2=\int_{a}^{b}(x-\mathbb{E}X)^2f(x)dx$$

::::
</div>

<div class="alert alert-success" style='background-color:white'>
:::: {.lightgreenframe data-latex=""}

**Propri√©t√©s :**

* $Var(X) \geq 0$
* $Var(X + Y) = Var[X] + Var[Y]\mbox{, (si } X\mbox{ et } Y\mbox{ ind√©p.)}$
* $Var(aX + b) = a^2Var(X)$

::::
</div>

<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

**L'√©cart-type** (en. *standard deviation, std*) de la v.a.r. $X$, not√© $\sqrt{Var(X)}$ ou $\sigma$, est une mesure de l‚Äô√©cart entre les valeurs prises par $X$ et son esp√©rance $\mathbb{E}X$

$\sigma(X)=\sigma_X = \sqrt{Var(X)}$

::::
</div>

Remarquons qu'il existe d'autres propri√©t√©s, dont une peut √™tre formul√©e sous forme de l'*in√©galit√© de Bienaym√©-Tchebychev* :

<div class="alert alert-warning">
:::: {.lightyellowbox data-latex=""}

**In√©galit√© de Bienaym√©-Tchebychev** : Soit $X$ une v.a.r. Alors :
$$\forall \alpha > 0, \alpha\in \mathbb{R} : \ \ \mathbb{P}(|X - \mathbb{E}X|\geq \alpha) \leq \frac{Var(X)}{\alpha^2}$$

::::
</div>

## Moments de l'ordre $p$

De la fa√ßon plus g√©n√©ralle, on peut d√©finir les moments comme suit :

<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

**Un moment d'ordre $p\ (p\in \mathbb{N})$** de la v.a.r. $X$ est un nombre r√©el $\mathbb{E}(|X|^p)$, quand il existe.

**Un moment centr√© d'ordre $p\ (p\in \mathbb{N})$** de la v.a.r. $X$ est un nombre r√©el $\mathbb{E}(|X - \mathbb{E}X|^p)$, quand il existe.

::::
</div>

## Exemple de calcul

Soit $X$ une v.a. d√©finie par les valeurs ci-dessous. Calculez sa variance et son √©cart-type.

<center>

\centering

![](img/example-variance-init.png){width=75%}

</center>

\raggedright

Il s'agit d'une v.a. discr√®te. V√©rifions que $\sum_{i=1}^n p(x_i) = 1$ :

$$\sum_{i=1}^n p(x_i) = 0.05 + 0.25 + 0.4 + 0.25 + 0.05 = 1$$

La variance d'une v.a. discr√®te est donn√©e par : 

$$Var(X) = \mathbb{E}[(X-\mathbb{E}X)^2]=\mathbb{E}[X^2]-(\mathbb{E}X)^2=\sum_i (x_i-\mathbb{E}X)^2 p_i$$

et son √©cart-type est d√©fini comme $\sigma(X) = \sqrt{Var(X)}$.

Ainsi, on peut voir que afin de calculer la variance et l'√©cart-type, il faut d'abord calculer l'esp√©rance de $X$, $\mathbb{E}X$, donn√©e par : 
$$\mathbb{E}[X] = \sum_i x_ip_i = \sum_i x_iP(X=x)$$

$$\mathbb{E}[X] = 1\times 0.05 + 2\times 0.25 + 3\times 0.4 + 4\times 0.25 + 5\times 0.05 = 0.05 + 0.5 + 1.2 + 1 + 0.25 = 3$$.

Soustrayons l'esp√©rance de $X$ √† partir des valeurs de $X$ et rajoutons une nouvelle ligne dans la table :


<center>

\centering

![](img/example-variance-1.png){width=75%}

</center>

\raggedright

Nous pouvons n√©anmoins appliquer la formule de la variance en multipliant les probabilit√©s de chaque valeurs par le nouveau terme qu'on vient de calculer et en prenant leur somme :

<center>

\centering

![](img/example-variance-2.png){width=75%}

</center>

\raggedright

$$Var(X) = \mathbf{4}\times 0.05 + \mathbf{1}\times 0.25 + \mathbf{0}\times 0.4 + \mathbf{1}\times 0.25 + \mathbf{4}\times 0.05 = 0.2 + 0.25 + 0 + 0.25 + 0.2 = \mathbf{0.9}$$

Une fois la variance est calcul√©e, il est possible de calculer l'√©cart-type :

$$\sigma(X) = \sqrt{Var(X)} = \mathbf{\sqrt{0.9}}$$

# Quelques exemples de lois de probabilit√©

Citons quelques examples de lois de probabilit√©.

## Distribution normale 

**La loi normale** aussi appel√©e **la loi de Gauss** (en. *Normal distribution*) est une loi de probabilit√© tr√®s √©tudi√©e et tr√®s utilis√©e. 

La notation suivante est utilis√©e : $X\sim \mathcal{N}(m, \sigma^2)$ ou $X\sim \mathcal{N}(\mu, \sigma^2)$ pour exprimer que la v.a.r. $X$ sur l'espace probabilis√© $(\Omega, \mathcal{A}, \mathbb{P})$ suit la loi normale avec les param√®tres $(m, \sigma^2)$ (ou $(\mu, \sigma^2)$). Le param√®tre $m$ ($\mu$) est l'esp√©rance de la distribution et le param√®tre $\sigma^2$ et la variance ($\sigma$ est l'√©cart-type).

La loi normale est d√©finie par *la fonction de densit√©* suivante :

$$f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-1/2\left(\frac{x-m}{\sigma}\right)^2}$$

*La fonction de r√©partition de la loi normale* est donc donn√©e par :
$$F(x) = \frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^x e^{-1/2\left(\frac{t-m}{\sigma}\right)^2}dt$$

La courbe de la fonction de densit√© a la forme dite de "cloche" et elle est sym√©trique par rapport √† $m$ :

<center>

\centering

![](img/normal-dist-mean(1).png)

</center>

\raggedright

Soit $X\sim \mathcal{N}(\mu, \sigma^2)$. On peut remarquer les r√©gularit√©s suivantes :

* $\mathbb{P}(\mu - \sigma\leq X \leq \mu + \sigma) \approx 0.682$
* $\mathbb{P}(\mu - 2\sigma\leq X \leq \mu + 2\sigma) \approx 0.954$
* $\mathbb{P}(\mu - 3\sigma\leq X \leq \mu + 3\sigma) \approx 0.997$

ou sur le graphique :

<center>

\centering

![](img/normal-percent-all.png){width=50%}

</center>

\raggedright

Observons maintenant deux courbes :

<center>

\centering

![](img/normal-diff-sigma-diff-mean.png)


</center>

\raggedright

Selon la forme et la position des centres, on peut d√©duire que : $\mu' > \mu$ (le centre est situ√© plus √† droite) et $\sigma' > \sigma$ (la courbe est plus √©tal√©e).

### Loi normale centr√©e r√©duite

Un cas sp√©cial de la loi normale est **la loi normale centr√©e r√©duite** (en. *standard normal distribution*). Il s'agit de la loi normale avec les param√®tres $(0,1)$, i.e. : $X\sim \mathcal{N}(0, 1)$, autrement dit $m=0$, $\sigma=1$.

Sa *fonction de densit√©* est :

$$\varphi = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$$ 

Visualisons-la :

<center>

\centering

![](img/standard-normal-1.png){width=50%}

</center>

\raggedright

Sa *fonction de r√©partition* est donn√©e par :

$$\Phi = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-t^2/2}dt$$

En la visualisant :

<center>
\centering

![](img/standard-normal-cdf.png)

</center>

\raggedright


Dans le `R`:

```{r}
# cr√©ation d'une s√©quence de valeurs dans la plage [-5,5] avec le pas 0.01
x <- seq(-5, 5, 0.01) 
# les valeurs de la fonction de densit√© de la loi normale centr√©e r√©duite 
# pour les x
y <- dnorm(x, mean=0, sd=1)
# visualisation de (x, y)
plot(x, y, main="La loi normale centr√©e r√©duite", ylab="f(x)")
```



### Z-table

Une des particularit√©s de la loi normale est que les valeurs num√©riques prises par sa fonction de r√©partition sont tabul√©es.

> Comment trouver la valeur de $\Phi(x)$ ?

Comme il est indiqu√© plus haut, les valeurs de la loi normale centr√©e r√©duite $\Phi(x) = \mathbb{P}(X\leq x)$ sont connues et on peut les trouver dans les tables.

<center>
\centering

![](img/normal-0.png){width=75%}

</center>

\raggedright

Observons la table de la loi normale centr√©e r√©duite.

`r getZtable()`

Supposons qu'on s'int√©resse √† $\Phi(1.53)$. 


<center>

\centering

`r gifImgCollection("img/z-table-example-2")`


</center>

\raggedright

Ainsi, on retrouve la valeur $\Phi(1.53) = 0.93699$.

Pour trouver la valeur de la fonction de r√©partition de la loi normale centr√©e r√©duite, on peut √©galement utiliser la fonction `pnorm(x, mean=0, sd=1)` de `R`. Reprenons le m√™me exemple de $\Phi(1.53)$ :

```{r}
# x=1.53
pnorm(1.53, mean=0, sd=1)
```

On obtient le m√™me r√©sultat.

**Et si on s'int√©resse √† une valeur n√©gative ?**

Dans ce cas l√†, gr√¢ce √† la sym√©trie de la loi normale, on peut utiliser la propri√©t√© suivante : pour $x < 0$, $\Phi(x) = 1 - \Phi(|x|)$. 

Regardons sur l'exemple de $x = -2$.

<center>

\centering

![](img/normal-for-2.png){width=50%}


</center>

\raggedright


Comme on peut voir √† partir de la table, $\Phi(2) = 0.99725$. Donc, $\Phi(-2) = 1 - 0.99725 = 0.00275$.

Dans le `R` :

```{r}
# x=-2
pnorm(-2, mean=0, sd=1)
# x=2
pnorm(2, mean=0, sd=1)
# 1-F(2)
1-pnorm(2, mean=0, sd=1)
# v√©rifions que F(-2) == 1-F(2) en arrondissant pour √©viter les diff√©rences 
# dans la pr√©cision de valeurs
round(pnorm(-2, mean=0, sd=1), 8) == round(1-pnorm(2, mean=0, sd=1), 8)
```


### Normalisation (standartisation)

> Comment trouver la valeur de $\mathbb{P}(X \leq x), \ X\sim \mathcal{N}(m, \sigma^2)$ si ce sont des valeurs de la loi normale centr√©e r√©duite qui sont tabul√©e ? 

Soit $X\sim \mathcal{N}(m, \sigma^2)$. Introduisons la v.a.r. $Z$ telle que :

$$Z = \frac{X-m}{\sigma}$$

Cette nouvelle v.a.r. suit la loi normale centr√©e r√©duite, i.e. $Z\sim \mathcal{N}(0,1)$.

Le processus de cr√©ation de $Z$ est appel√© **standartisation** ou **normalisation** (en. *normalisation* ou *standartisation*). 

Notons une propri√©t√© importante :

$$\mathbb{P}(X \leq x) = \mathbb{P}\left(\frac{X-m}{\sigma}\leq\frac{x-m}{\sigma}\right) = \mathbb{P}\left(Z\leq \frac{x-m}{\sigma}\right)$$
Alors, en se servant de la table des valeurs pour la loi normale centr√©r r√©duite, on peut trouver $\mathbb{P}(X \leq x)$.

## Autres lois de probabilit√©

Pr√©sentons ici quelques lois de probabilit√© avec leurs caract√©ristiques principales.

### Variables discr√®tes

||Plage ds valeurs|Fonction de masse|Visualisation|Esp√©rance|Variance|Interpr√©tation|
|---------|:--:|:--------------------:|:-------:|:-----:|:-----:|----------------|
|**Uniforme discr√®te**, $\mathcal{U}(N)$| $\{1,...,N\}$| $$\mathbb{P}(X=k) = \frac{1}{N}$$|![](img/dist-discrete-uniform.png)|$\frac{N+1}{2}$|$\frac{N^2-1}{12}$| $N$ issues √©quiprobables|
|**Bernoulli**, $\mathcal{B}(p)$| $\{0,1\}$ | $$\mathbb{P}(X=k) = \left\{\begin{array}{ll}1-p, \mbox{ si } k=0 \\ p, \mbox{ si } k=1 \\ 0, \mbox{ sinon}\end{array}\right.$$||$p$|$p(1-p)$|2 issues possibles dont une avec la probabilit√© $p$|
|**Binomiale**, $\mathcal{B}(n,p)$| $\{0,...,n\}$ | $$\mathbb{P}(X=k) = C^k_n p^k(1-p)^{n-k}$$|![](img/dist-discrete-binomial.png)|$np$|$np(1-p)$|Somme de $n$ Bernoullis ind√©pendants : \# succ√®s dans $n$ tirages si chaque tirage a une probabilit√© $p$ d'√™tre gagnant (e.g. \# tickets gagnant parmi $n$)|
|**G√©om√©trique**, $\mathcal{G}(p)$| $\{0,...,\infty\}$| $$\mathbb{P}(X=k) = p(1-p)^{k-1}$$ |![](img/dist-discrete-geometric.png)| $\frac{1}{p}$ | $\frac{1-p}{p^2}$|\# tirages avant le 1er succ√®s dans une s√©quence de Bernoullis ind√©pendants (e.g. \# piles avant la 1√®re face)|
|**Poisson**, $\mathcal{P}(\lambda)$|$\{0,...,\infty\}$| $$\mathbb{P}(X=k) = e^{-\lambda\frac{\lambda^k}{k!}}$$ | ![](img/dist-discrete-poisson.png) | $\lambda$ | $\lambda$ | Ev√®nements rares : une r√©alisation sur un grand nombre d'exp√©riences|


### Variables continues

||Plage ds valeurs|Densit√©|Visualisation|Esp√©rance|Variance|Interpr√©tation|
|---------|:--:|:--------------------:|:-------:|:-----:|:-----:|----------------|
|**Uniforme**, $\mathcal{U}([a,b])$| $[a,b]$| $$f(x) = \frac{1}{b-a}\mathbb{I}_{[a,b]}(x)$$ $\mathbb{I}_{A}(x) = \left\{\begin{array}{ll} 1, \mbox{ si } x\in A \\ 0, \mbox{ sinon} \end{array} \right.$ |![](img/dist-cont-uniform.png) | $\frac{a+b}{2}$ | $\frac{(b-a)^2}{12}$ | Toutes les valeurs ont la m√™me chance d‚Äôappara√Ætre (permutation al√©atoire uniforme) |
| **Normale, Gausienne, Laplace-Gauss**, $\mathcal{N}(m,\sigma^2)$ | $\mathbb{R}$ | $$f(x) =  \frac{1}{\sigma\sqrt{2\pi}} e^{-1/2\left(\frac{x-m}{\sigma}\right)^2}$$| ![](img/dist-cont-normal.png)| $m$ | $\sigma^2$ | Loi des moyennes |
| **Exponentielle**, $\mathcal{E}(\lambda)$ | $\mathbb{R}^{+}$ | $$f(x) = \lambda e^{-\lambda x}\mathbb{I}_{\mathbb{R}^{+}}(x)$$ | ![](img/dist-cont-exp.png) | $\frac{1}{\lambda}$ | $\frac{1}{\lambda^2}$ | Loi des dur√©s de vie / r√©alisations de t√¢ches |


# Fonction caract√©ristique et fonction g√©n√©ratrice

> Soit $X$ v.a.r. de la loi connue, soit $h : \mathbb{R} \rightarrow \mathbb{R}$ fonction continue par morceaux. Quelle est la loi de la v.a.r. $X$ ?

Afin de pouvoir r√©pondre √† ce genre de probl√®mes, d'autres caract√©risations de la loi d'une v.a.r. √† part la fonction de masse / densit√© peuvent √™tre utiles. 

Ici, nous pr√©senterons bri√®vemet la fonction caract√©ristique et la fonction g√©n√©ratrice. Pour plus de d√©tails, voir @balac.

## Fonction caract√©ristique

<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

On appelle **la fonction caract√©ristique** de la v.a.r. $X$ la fonction de $\mathbb{R}$ dans $\mathbb{C}$ d√©finie par $$\phi_X(t) = \mathbb{E}(e^{itX}), \ \forall t\in \mathbb{R}$$ 

*Quelques propri√©t√©s* : 

* Si $\phi_X(t)$ est deux fois d√©rivable en 0, alors $\mathbb{E}(X)$ et $\mathbb{E}(X^2)$ existent et :

$$\mathbb{E}(X) = -i\phi'_X(0)$$
$$\mathbb{E}(X^2) = -\phi''_X(0)$$

* Soit $a$ et $b$ deux r√©els et $Y$ la v.a.r. d√©finie par $Y = aX + b$. La fonction caract√©ristique  $\phi_Y(t)$
de la v.a.r. $Y$ v√©rifie :

$$\phi_Y(t) = e^{itb}\phi_X(at), \ \forall t\in \mathbb{R}$$

::::
</div>

## Fonction g√©n√©ratrice
<div class="alert alert-success">
:::: {.lightgreenbox data-latex=""}

Soit $X$ v.a.r. discr√®te √† valeurs enti√®res. On appelle **fonction g√©n√©ratrice de $X$** la fonction d√©finie pour tout $s\in [-1,1]$ par 
$$G_X(s) = \mathbb{E}(s^X) = \sum_{j\in \mathbb{N}}s^j \times \mathbb{P}(X=j)$$


*Quelques propri√©t√©s *:

* $G_X(1) = 1$, $G_X(0) = \mathbb{P}(X=0)$
* $G'_X(1) = \mathbb{E}(X)$, $G''_X(1) = \mathbb{E}(X(X-1))$ et plus g√©n√©ralement : $\forall k\in \mathbb{N}^*,$

$$G^{(k)}_X(1) = \mathbb{E}(X\times (X-1)\times ...\times (X-k+1))$$

::::
</div>

# Liens utiles 

1. Jeremy Orloff, and Jonathan Bloom. *18.05 Introduction to Probability and Statistics*. Spring 2014. Massachusetts Institute of Technology: MIT OpenCourseWare, [https://ocw.mit.edu](https://ocw.mit.edu). License: Creative Commons BY-NC-SA.
2. [Combinaisons avec r√©p√©tition](https://www.youtube.com/watch?v=1crNnZkIvac) 
3. [Z-table](https://www.ztable.net/)

# References

